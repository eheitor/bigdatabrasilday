1
00:00:00,000 --> 00:00:02,145
So, just as in the previous lab,

2
00:00:02,145 --> 00:00:04,740
we need to go ahead and start up Datalab.

3
00:00:04,740 --> 00:00:06,225
Once we are in Datalab,

4
00:00:06,225 --> 00:00:08,130
go ahead and clone the notebooks.

5
00:00:08,130 --> 00:00:10,860
While we're in the labs folder in the notebook,

6
00:00:10,860 --> 00:00:13,980
I'll going into the labs folder and I'm looking

7
00:00:13,980 --> 00:00:18,615
for two hyphens sample, two _sample.IPythonnotebook.

8
00:00:18,615 --> 00:00:22,335
So, this is the notebook that we're going to be working in

9
00:00:22,335 --> 00:00:27,090
and our job now is to create a sample dataset.

10
00:00:27,090 --> 00:00:30,240
We're starting with the data that isn't BigQuery,

11
00:00:30,240 --> 00:00:35,835
the data that we explored in the previous lab and we want to basically take that data.

12
00:00:35,835 --> 00:00:37,980
We want to create a small datasets,

13
00:00:37,980 --> 00:00:41,520
so that we can develop TensorFlow models on it.

14
00:00:41,520 --> 00:00:45,425
The goal that we are given is that we want to create about

15
00:00:45,425 --> 00:00:50,420
12,000 training examples and about 3,000 evaluation examples.

16
00:00:50,420 --> 00:00:52,195
This is the full dataset.

17
00:00:52,195 --> 00:00:54,615
So, but before we do this,

18
00:00:54,615 --> 00:01:01,220
let's go ahead and make sure that we have a project and a bucket in Qwiklabs,

19
00:01:01,220 --> 00:01:02,540
when we created Qwiklabs,

20
00:01:02,540 --> 00:01:05,680
we would have gotten, in my case,

21
00:01:05,680 --> 00:01:07,440
this is a project ID,

22
00:01:07,440 --> 00:01:12,155
so I can go ahead and go down here and get the project ID.

23
00:01:12,155 --> 00:01:16,510
That is what I'm going to paste in as a project ID.

24
00:01:16,510 --> 00:01:19,275
Here, that is my project ID.

25
00:01:19,275 --> 00:01:23,430
A bucket, I can go into the GCP Console,

26
00:01:23,430 --> 00:01:26,320
go down to Storage.

27
00:01:29,720 --> 00:01:33,570
In Storage, I can go to Browser.

28
00:01:33,570 --> 00:01:36,230
At this point, there is no bucket created,

29
00:01:36,230 --> 00:01:40,940
so I can create a bucket and it needs to be unique across Cloud Storage.

30
00:01:40,940 --> 00:01:43,475
How am I going to get a unique bucket name?

31
00:01:43,475 --> 00:01:46,800
A pretty good thing to try, is a project name.

32
00:01:46,800 --> 00:01:48,360
Your project name's a unique,

33
00:01:48,360 --> 00:01:52,400
unless you're really unlucky nobody's used this project name Azure bucket

34
00:01:52,400 --> 00:01:56,630
name and the UI tells you if it has already been used.

35
00:01:56,630 --> 00:01:57,965
In this case, it hasn't,

36
00:01:57,965 --> 00:02:00,620
so I can use the project name Azure bucket name,

37
00:02:00,620 --> 00:02:04,685
multi-regional is fine and I'll go ahead and create the bucket.

38
00:02:04,685 --> 00:02:08,105
At this point, the bucket has been created.

39
00:02:08,105 --> 00:02:13,420
There is a bucket, so we can go back to our notebook and when it asks for the bucket,

40
00:02:13,420 --> 00:02:16,170
we will specify that bucket name.

41
00:02:16,170 --> 00:02:18,640
When I created my Datalab VM,

42
00:02:18,640 --> 00:02:22,400
I created it in the US Central one zone.

43
00:02:22,400 --> 00:02:25,535
So, US Central 1B is what I think I used,

44
00:02:25,535 --> 00:02:28,255
it doesn't matter, but the region is US Central one,

45
00:02:28,255 --> 00:02:29,730
so I can go ahead and use that.

46
00:02:29,730 --> 00:02:32,345
So, at this point, I can run this cell

47
00:02:32,345 --> 00:02:35,420
and the bucket project and region variables have been set,

48
00:02:35,420 --> 00:02:37,640
but those are set for Python.

49
00:02:37,640 --> 00:02:41,269
We would also like in general to set it for Bash,

50
00:02:41,269 --> 00:02:44,840
and in order to do that we can use the os.environment

51
00:02:44,840 --> 00:02:48,770
call in Python to set three different environment variables,

52
00:02:48,770 --> 00:02:50,480
bucket project and region,

53
00:02:50,480 --> 00:02:54,350
so that when we call Bash scripts from our notebook,

54
00:02:54,350 --> 00:02:56,320
these three things are defined as well.

55
00:02:56,320 --> 00:02:59,520
I can hit run or I can hit Shift-Enter.

56
00:02:59,520 --> 00:03:02,555
I will do Shift-Enter because it's muscle memory for me.

57
00:03:02,555 --> 00:03:05,120
Having done that, in my case,

58
00:03:05,120 --> 00:03:10,910
I went ahead and did the gsutil make bucket from the Cloud Console.

59
00:03:10,910 --> 00:03:14,260
This is little bit of a defensive programming on our part,

60
00:03:14,260 --> 00:03:16,100
if you didn't create the bucket,

61
00:03:16,100 --> 00:03:19,230
we tried to create a bucket with the appropriate name.

62
00:03:19,230 --> 00:03:22,660
So, you could have done this and this would also create the bucket.

63
00:03:22,660 --> 00:03:24,080
If the bucket doesn't exist,

64
00:03:24,080 --> 00:03:25,745
it creates a bucket.

65
00:03:25,745 --> 00:03:30,830
Now, we can create the machine-learning dataset by sampling using BigQuery.

66
00:03:30,830 --> 00:03:32,195
This is what we need to do,

67
00:03:32,195 --> 00:03:35,860
and we are given a SQL query to start with.

68
00:03:35,860 --> 00:03:41,000
So, this is the data that we've been exploring so far in lab number one,

69
00:03:41,000 --> 00:03:43,220
and if I do this at this point,

70
00:03:43,220 --> 00:03:47,470
all that I have gotten is that I've set a string variable set.

71
00:03:47,470 --> 00:03:51,140
What I would really like to do is to run this query,

72
00:03:51,140 --> 00:03:53,940
to execute this query from within Python.

73
00:03:53,940 --> 00:03:56,630
So, let's go ahead and create a new code cell,

74
00:03:56,630 --> 00:03:58,535
and that's the new code cell up here,

75
00:03:58,535 --> 00:03:59,930
appended new code cells.

76
00:03:59,930 --> 00:04:01,270
So, I click on that,

77
00:04:01,270 --> 00:04:03,560
it is here I would like to move it down,

78
00:04:03,560 --> 00:04:04,775
so I can move it down.

79
00:04:04,775 --> 00:04:07,685
So, that this is in lab task number one.

80
00:04:07,685 --> 00:04:11,300
If I want to run this Datalab command,

81
00:04:11,300 --> 00:04:17,280
I will can basically use the BigQuery package in Datalab.

82
00:04:17,280 --> 00:04:18,760
I never remember it,

83
00:04:18,760 --> 00:04:22,835
I will go ahead and import that package,

84
00:04:22,835 --> 00:04:24,965
I wonder if it's already imported,

85
00:04:24,965 --> 00:04:26,775
it was already imported for us.

86
00:04:26,775 --> 00:04:28,855
So, there was a BigQuery import,

87
00:04:28,855 --> 00:04:35,165
and the other thing is to basically call bq.Query and call the execute method.

88
00:04:35,165 --> 00:04:36,815
So, let's go ahead and do this.

89
00:04:36,815 --> 00:04:44,250
So, go down here and we can say bq.Query and we do query and we can do execute.

90
00:04:44,250 --> 00:04:45,805
This we'll execute the query,

91
00:04:45,805 --> 00:04:49,535
and we can take the result and make it a to_dataframe.

92
00:04:49,535 --> 00:04:52,580
But before I run this, think about this.

93
00:04:52,580 --> 00:04:55,955
How many rows is this query going to return?

94
00:04:55,955 --> 00:05:00,540
I'm just doing a select and I'm doing a where you're greater than 2,000,

95
00:05:00,540 --> 00:05:02,770
it's going to give me a boatload of numbers,

96
00:05:02,770 --> 00:05:04,000
I don't want all of those.

97
00:05:04,000 --> 00:05:06,460
So, what I will do, is I will say,

98
00:05:06,460 --> 00:05:12,855
select count of maybe weight in pounds,

99
00:05:12,855 --> 00:05:20,755
FROM, and I'll do an inner query of that original query plus that.

100
00:05:20,755 --> 00:05:23,930
So, maybe you're wondering what exactly does this do.

101
00:05:23,930 --> 00:05:26,930
Let's just make sure that this is correct.

102
00:05:26,930 --> 00:05:29,555
I can go ahead and make a new code cell,

103
00:05:29,555 --> 00:05:33,095
move it up and try to print that.

104
00:05:33,095 --> 00:05:36,600
So, what does this do, did I get it right?

105
00:05:36,650 --> 00:05:38,970
I did get it right.

106
00:05:38,970 --> 00:05:45,015
So, BigQuery, select count of weight in pounds from,

107
00:05:45,015 --> 00:05:47,820
I need a plus here, do that.

108
00:05:47,820 --> 00:05:51,380
So now, it's selecting the number of

109
00:05:51,380 --> 00:05:55,260
rows that have weight in pounds from this original query.

110
00:05:55,260 --> 00:05:57,635
So, this is the one that we really want to run.

111
00:05:57,635 --> 00:05:59,465
So, let's go ahead and do that.

112
00:05:59,465 --> 00:06:02,740
So, that is a query that I want to execute.

113
00:06:02,740 --> 00:06:07,910
When I execute it, I will get back a DataFrame, that is df.

114
00:06:07,910 --> 00:06:10,445
We can go ahead and print df.

115
00:06:10,445 --> 00:06:12,335
So, let's do this.

116
00:06:12,335 --> 00:06:15,155
At this point, the BigQuery queries running,

117
00:06:15,155 --> 00:06:17,990
and if we just run that query,

118
00:06:17,990 --> 00:06:19,550
we would get back.

119
00:06:19,550 --> 00:06:21,860
How many rows is that?

120
00:06:21,860 --> 00:06:23,765
That is 33 million rows,

121
00:06:23,765 --> 00:06:25,370
we want 12,000 of them.

122
00:06:25,370 --> 00:06:29,695
So, we need to somehow reduce that number. How would I do it?

123
00:06:29,695 --> 00:06:38,490
Well, so let's go ahead and call this my sampling query equals that.

124
00:06:38,490 --> 00:06:42,834
So, this way we can make sure that we are actually doing the query correct.

125
00:06:42,834 --> 00:06:45,000
So, that's why sampling query.

126
00:06:45,000 --> 00:06:47,925
I can print the sampling query here,

127
00:06:47,925 --> 00:06:50,140
and I can change this to be sampling query.

128
00:06:50,140 --> 00:06:53,280
I haven't changed anything until this point.

129
00:06:53,560 --> 00:06:56,555
So, I could add a where clause.

130
00:06:56,555 --> 00:07:00,900
Where for example, this hashmonth,

131
00:07:00,900 --> 00:07:04,310
that is a hashed year and month.

132
00:07:04,310 --> 00:07:07,490
What I could do is I could say where hash of

133
00:07:07,490 --> 00:07:12,610
the modulo of the absolute value of the hashmonth,

134
00:07:12,610 --> 00:07:20,640
is if you modulo it by say 10 is less than eight.

135
00:07:20,640 --> 00:07:23,220
That'll be 80 percent of the data you think.

136
00:07:23,220 --> 00:07:26,430
If I do that, what do I get?

137
00:07:26,430 --> 00:07:28,215
If I now print it,

138
00:07:28,215 --> 00:07:31,150
how many rows will I get?

139
00:07:38,530 --> 00:07:42,255
I would get 80 percent of the rows.

140
00:07:42,255 --> 00:07:45,070
So, this is good as far as creating

141
00:07:45,070 --> 00:07:49,090
a training dataset versus an evaluation dataset is concern,

142
00:07:49,090 --> 00:07:50,815
but I don't want this many rows.

143
00:07:50,815 --> 00:07:52,595
I want far fewer rows.

144
00:07:52,595 --> 00:07:53,985
So, how could I do that?

145
00:07:53,985 --> 00:07:55,919
In addition to the modulo,

146
00:07:55,919 --> 00:08:00,165
I could do a RAND and RAND less than.

147
00:08:00,165 --> 00:08:02,755
Now, think about how much I need to sample it by.

148
00:08:02,755 --> 00:08:05,815
If I sampled 1,000,

149
00:08:05,815 --> 00:08:08,900
I would get about 27,000.

150
00:08:08,900 --> 00:08:13,080
So, if I say RAND less than 0.001,

151
00:08:13,080 --> 00:08:14,505
that is one in a thousand,

152
00:08:14,505 --> 00:08:18,150
I would get about 27,000, I want 12,000.

153
00:08:18,150 --> 00:08:19,380
So, I could basically,

154
00:08:19,380 --> 00:08:20,670
if I do that,

155
00:08:20,670 --> 00:08:23,835
I would get 2,500.

156
00:08:23,835 --> 00:08:26,040
So, multiply that by four,

157
00:08:26,040 --> 00:08:28,380
and I should get about 12,000.

158
00:08:28,380 --> 00:08:31,650
That's about 10,000, let see how much I get. So, I can do that.

159
00:08:31,650 --> 00:08:33,300
That's my sampling query,

160
00:08:33,300 --> 00:08:36,220
and if I do my df.

161
00:08:38,910 --> 00:08:42,730
So now I get about 11,000 rows.

162
00:08:42,730 --> 00:08:46,255
So that seems to be a reasonably sized thing.

163
00:08:46,255 --> 00:08:49,450
Of course, I can go back here and change this.

164
00:08:49,450 --> 00:08:50,530
Instead of less than eight,

165
00:08:50,530 --> 00:08:52,990
I can say greater than or equal to eight,

166
00:08:52,990 --> 00:08:54,355
and I will get the remainder,

167
00:08:54,355 --> 00:08:56,780
and it should be about 20 percent.

168
00:08:56,780 --> 00:08:59,670
That is one way that you could do the sampling.

169
00:08:59,670 --> 00:09:02,360
We could of course do this to be four,

170
00:09:02,360 --> 00:09:07,075
and this to be three to get 75 percent of the data's training and very similar thing.

171
00:09:07,075 --> 00:09:08,170
You can play around with it,

172
00:09:08,170 --> 00:09:12,685
but this gives you the idea of how you would go ahead and do the sampling.

173
00:09:12,685 --> 00:09:14,890
The next thing that we need to do,

174
00:09:14,890 --> 00:09:19,630
is that we want to go ahead and use Pandas to clean up the data,

175
00:09:19,630 --> 00:09:24,265
to remove rows that are missing fields, right?

176
00:09:24,265 --> 00:09:26,425
Why would there be missing fields?

177
00:09:26,425 --> 00:09:28,330
This may be my sampling query,

178
00:09:28,330 --> 00:09:32,905
but let's go ahead and change this now,

179
00:09:32,905 --> 00:09:35,185
to basically get the actual data.

180
00:09:35,185 --> 00:09:36,910
This is now just doing the count.

181
00:09:36,910 --> 00:09:40,810
What I really need to do is to actually get the data itself.

182
00:09:40,810 --> 00:09:42,475
I'll change this thing,

183
00:09:42,475 --> 00:09:44,545
maybe I'll just start a new query.

184
00:09:44,545 --> 00:09:53,400
So there, and that is basically what we wanted, query equals that.

185
00:09:53,400 --> 00:09:58,905
The three quotes in Python lets me do a multi-line string, right?

186
00:09:58,905 --> 00:10:04,450
Where? Here 2,000 and that.

187
00:10:06,060 --> 00:10:10,960
This is now by sampling query,

188
00:10:10,960 --> 00:10:19,190
which is selecting all of these columns where all of these conditions are met.

189
00:10:29,280 --> 00:10:34,435
At this point I have my query and I can do the same thing,

190
00:10:34,435 --> 00:10:39,565
but this time I will get 12,000 rows instead.

191
00:10:39,565 --> 00:10:42,040
I don't want to get all 12,000.

192
00:10:42,040 --> 00:10:52,180
I cannot do this

193
00:10:52,180 --> 00:10:54,220
because hash month is an alias,

194
00:10:54,220 --> 00:10:56,230
I cannot use it in the where class.

195
00:10:56,230 --> 00:10:58,690
I will need to do what I did before,

196
00:10:58,690 --> 00:11:03,380
select star from all of these things,

197
00:11:07,410 --> 00:11:11,350
and that should do it.

198
00:11:11,350 --> 00:11:12,700
This is basically what I did.

199
00:11:12,700 --> 00:11:16,630
I said select all of these fields with this new thing.

200
00:11:16,630 --> 00:11:19,585
Now I have my data,

201
00:11:19,585 --> 00:11:23,875
these things seem to be fine.

202
00:11:23,875 --> 00:11:25,420
Is male seems to have values,

203
00:11:25,420 --> 00:11:28,240
mother's age seems to have values, all of these things.

204
00:11:28,240 --> 00:11:32,275
The question seems to suggest that some rows are missing some fields.

205
00:11:32,275 --> 00:11:33,985
Let's go ahead and try this.

206
00:11:33,985 --> 00:11:36,325
Instead of doing the F dot head,

207
00:11:36,325 --> 00:11:38,710
actually let's move these things down,

208
00:11:38,710 --> 00:11:47,290
because this is actually for the second thing.

209
00:11:47,290 --> 00:11:49,120
Okay, there we go,

210
00:11:49,120 --> 00:11:57,820
and let's go ahead and add a new thing, DF dot describe.

211
00:11:57,820 --> 00:12:01,855
This gives us statistics on the numeric columns.

212
00:12:01,855 --> 00:12:03,685
If we do that,

213
00:12:03,685 --> 00:12:09,490
we realize that the count is 10,974 for the weight in pounds,

214
00:12:09,490 --> 00:12:11,830
but 10,984 for mother's age.

215
00:12:11,830 --> 00:12:14,590
The number of rows is different for the different columns,

216
00:12:14,590 --> 00:12:16,150
that's because of [inaudible] ,

217
00:12:16,150 --> 00:12:17,740
and we want to prevent that.

218
00:12:17,740 --> 00:12:19,735
What we would have to do here,

219
00:12:19,735 --> 00:12:23,320
is that in addition to your greater than zero, we'd have to say,

220
00:12:23,320 --> 00:12:31,480
and is male is not null, and so on.

221
00:12:31,480 --> 00:12:35,780
Plurality is not null.

222
00:12:45,690 --> 00:12:49,135
If I now do a describe.

223
00:12:49,135 --> 00:12:54,970
we now see that mother's age and plurality are both 10,833,

224
00:12:54,970 --> 00:12:56,905
that is the total number of rows.

225
00:12:56,905 --> 00:13:00,385
Weight in pounds 10,823 is a problem,

226
00:13:00,385 --> 00:13:02,725
we need to go back here and add,

227
00:13:02,725 --> 00:13:08,155
and weight in pounds is not null.

228
00:13:08,155 --> 00:13:10,870
You would do this for all of these columns,

229
00:13:10,870 --> 00:13:16,735
until all of them actually have the exact same number of rows that you need.

230
00:13:16,735 --> 00:13:19,345
Then what else are we asked to do?

231
00:13:19,345 --> 00:13:22,960
We are asked to simulate the lack of ultrasound.

232
00:13:22,960 --> 00:13:24,205
This is trickier.

233
00:13:24,205 --> 00:13:26,935
How would you not have ultrasound?

234
00:13:26,935 --> 00:13:30,295
Well, if you don't have ultrasound, we're saying that,

235
00:13:30,295 --> 00:13:35,185
the is mail column is unknown, right?

236
00:13:35,185 --> 00:13:39,805
We'd have to basically create a second data set. Let's do this.

237
00:13:39,805 --> 00:13:41,665
What we could do here,

238
00:13:41,665 --> 00:13:44,230
is we could make a deep copy,

239
00:13:44,230 --> 00:13:48,760
and we can say DF two equals copy.

240
00:13:48,760 --> 00:13:51,475
I would have to import copy, I believe.

241
00:13:51,475 --> 00:13:59,320
Import copy, and so DF two equals copy dot deep copy of DF,

242
00:13:59,320 --> 00:14:01,640
I think that's right.

243
00:14:03,020 --> 00:14:06,395
Yes, I think so. All right.

244
00:14:06,395 --> 00:14:11,020
Let's do DF dot head, DF two dot head.

245
00:14:11,020 --> 00:14:13,540
At this point, it's exactly the same data,

246
00:14:13,540 --> 00:14:16,990
but what I can do is in df2,

247
00:14:16,990 --> 00:14:22,465
I can set the is mail to be unknown.

248
00:14:22,465 --> 00:14:26,125
This is the case where there is no ultrasound.

249
00:14:26,125 --> 00:14:27,535
If I do that,

250
00:14:27,535 --> 00:14:30,610
now the is male columns are all unknown,

251
00:14:30,610 --> 00:14:34,285
but DF dot head should still be fine, hopefully.

252
00:14:34,285 --> 00:14:36,835
DF dot head is still fine, right?

253
00:14:36,835 --> 00:14:40,585
Now we basically have two data sets.

254
00:14:40,585 --> 00:14:42,925
This is how you change the Is mail column,

255
00:14:42,925 --> 00:14:45,655
and you would do the same thing for plurality.

256
00:14:45,655 --> 00:14:49,975
Plurality would be single or multiple.

257
00:14:49,975 --> 00:14:56,875
Single if, of a certain case, else multiple, right?

258
00:14:56,875 --> 00:14:58,630
In order to be able to do that,

259
00:14:58,630 --> 00:15:01,510
you'd have to actually go ahead and check.

260
00:15:01,510 --> 00:15:04,344
It is not DF of plurality for everything,

261
00:15:04,344 --> 00:15:14,245
but DF of plurality on those rows where DF two currently of plurality,

262
00:15:14,245 --> 00:15:18,110
is equal to one, is single.

263
00:15:18,150 --> 00:15:27,145
At this point, now we're basically changing that plurality where of lock,

264
00:15:27,145 --> 00:15:28,810
or something like this, right?

265
00:15:28,810 --> 00:15:33,610
Go ahead and get the location wherever the plurality columns are one,

266
00:15:33,610 --> 00:15:36,655
and then change those values to single,

267
00:15:36,655 --> 00:15:38,860
and you do the same thing for multiple.

268
00:15:38,860 --> 00:15:44,200
The final thing, is to basically go ahead and write it all out.

269
00:15:44,200 --> 00:15:48,250
In my case, I have DF and I have DF two.

270
00:15:48,250 --> 00:15:50,515
I basically merged the two,

271
00:15:50,515 --> 00:15:53,650
and then I would basically call it train DF and write it out,

272
00:15:53,650 --> 00:15:56,290
and you would do the same thing for eval DF.

273
00:15:56,290 --> 00:16:01,000
With that, we should have a training dot CSV file,

274
00:16:01,000 --> 00:16:03,175
and the evaluation dot CSV file,

275
00:16:03,175 --> 00:16:07,110
that we can now use to create a TensorFlow model.
